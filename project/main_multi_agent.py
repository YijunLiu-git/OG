# main_multi_agent.py - ä¿®å¤ç‰ˆ

import os
import yaml
import torch
import numpy as np
import networkx as nx
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime
from typing import Dict, List, Any
import argparse
import random

from env.vnf_env_multi import MultiVNFEmbeddingEnv
from env.topology_loader import generate_topology
from agents.base_agent import create_agent
from utils.logger import Logger
from utils.metrics import calculate_sar, calculate_splat
from config_loader import get_scenario_config, print_scenario_plan, validate_all_configs, load_config

class MultiAgentTrainer:
    """
    å¤šæ™ºèƒ½ä½“VNFåµŒå…¥è®­ç»ƒå™¨ - ä¿®å¤ç‰ˆæœ¬
    
    ä¸»è¦ä¿®å¤ï¼š
    1. âœ… ç¡®ä¿ç»´åº¦ä¸€è‡´æ€§
    2. âœ… ç®€åŒ–åœºæ™¯é…ç½®é€»è¾‘
    3. âœ… æ”¹è¿›é”™è¯¯å¤„ç†
    4. âœ… æ·»åŠ æ›´è¯¦ç»†çš„æ—¥å¿—è®°å½•
    """
    
    def __init__(self, config_path: str = "config.yaml"):
        print("ğŸš€ åˆå§‹åŒ–å¤šæ™ºèƒ½ä½“è®­ç»ƒå™¨...")
        
        # åŠ è½½é…ç½®
        self.config = load_config(config_path)
        
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # è®­ç»ƒå‚æ•°
        self.episodes = self.config['train']['episodes']
        self.save_interval = 50
        self.eval_interval = 25
        self.agent_types = ['ddqn', 'dqn', 'ppo']
        
        # åˆ›å»ºç»“æœç›®å½•
        self.results_dir = "results"
        os.makedirs(self.results_dir, exist_ok=True)
        
        # åœºæ™¯ç›¸å…³
        self.current_scenario = "normal_operation"
        self.scenario_start_episode = 1
        self.last_applied_scenario = None
        
        # åˆå§‹åŒ–ç»„ä»¶
        self._setup_network_topology()
        self._setup_environments()
        self._setup_agents()
        self._setup_logging()
        
        print(f"âœ… å¤šæ™ºèƒ½ä½“è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   - æ™ºèƒ½ä½“ç±»å‹: {self.agent_types}")
        print(f"   - è®­ç»ƒè½®æ•°: {self.episodes}")
        print(f"   - ç½‘ç»œèŠ‚ç‚¹: {len(self.graph.nodes())}")
    
    def _setup_network_topology(self):
        """è®¾ç½®ç½‘ç»œæ‹“æ‰‘"""
        print("ğŸŒ è®¾ç½®ç½‘ç»œæ‹“æ‰‘...")
        
        try:
            # ä½¿ç”¨å®Œæ•´çš„é…ç½®å­—å…¸ç”Ÿæˆæ‹“æ‰‘
            full_config = {
                'topology': self.config['topology'],
                'vnf_requirements': self.config['vnf_requirements'],
                'dimensions': self.config['dimensions']
            }
            
            self.graph, self.node_features, self.edge_features = generate_topology(config=full_config)
            
            # âœ… éªŒè¯æ‹“æ‰‘ç”Ÿæˆç»“æœ
            num_nodes = len(self.graph.nodes())
            num_edges = len(self.graph.edges())
            
            # ç¡®ä¿è¿é€šæ€§
            if not nx.is_connected(self.graph):
                print("âš ï¸ å›¾ä¸è¿é€šï¼Œå°è¯•æ·»åŠ è¾¹...")
                # ç®€å•å¤„ç†ï¼šè¿æ¥æ‰€æœ‰å­¤ç«‹çš„ç»„ä»¶
                components = list(nx.connected_components(self.graph))
                for i in range(len(components) - 1):
                    node1 = list(components[i])[0]
                    node2 = list(components[i + 1])[0]
                    self.graph.add_edge(node1, node2)
                    # ä¸ºæ–°è¾¹æ·»åŠ ç‰¹å¾
                    new_edge_features = np.array([[50.0, 5.0, 0.1, 0.01]])  # é»˜è®¤è¾¹ç‰¹å¾
                    self.edge_features = np.vstack([self.edge_features, new_edge_features])
            
            # éªŒè¯ç‰¹å¾ç»´åº¦
            expected_node_dim = 4  # åŸºç¡€ç‰¹å¾ç»´åº¦
            expected_edge_dim = 4  # è¾¹ç‰¹å¾ç»´åº¦
            
            assert self.node_features.shape[1] == expected_node_dim, \
                f"èŠ‚ç‚¹ç‰¹å¾åº”ä¸º{expected_node_dim}ç»´ï¼Œå®é™…{self.node_features.shape[1]}ç»´"
            assert self.edge_features.shape[1] == expected_edge_dim, \
                f"è¾¹ç‰¹å¾åº”ä¸º{expected_edge_dim}ç»´ï¼Œå®é™…{self.edge_features.shape[1]}ç»´"
            
            print(f"âœ… ç½‘ç»œæ‹“æ‰‘ç”Ÿæˆå®Œæˆ:")
            print(f"   - èŠ‚ç‚¹æ•°: {num_nodes}")
            print(f"   - è¾¹æ•°: {len(self.graph.edges())}")
            print(f"   - è¿é€šæ€§: {nx.is_connected(self.graph)}")
            print(f"   - èŠ‚ç‚¹ç‰¹å¾: {self.node_features.shape}")
            print(f"   - è¾¹ç‰¹å¾: {self.edge_features.shape}")
            
        except Exception as e:
            print(f"âŒ ç½‘ç»œæ‹“æ‰‘è®¾ç½®å¤±è´¥: {e}")
            raise
    
    def _setup_environments(self):
        """è®¾ç½®è®­ç»ƒå’Œæµ‹è¯•ç¯å¢ƒ"""
        print("ğŸŒ è®¾ç½®è®­ç»ƒç¯å¢ƒ...")
        
        try:
            reward_config = self.config['reward']
            chain_length_range = tuple(self.config['vnf_requirements']['chain_length_range'])
            
            # ç¯å¢ƒé…ç½®
            env_config = {
                'topology': self.config['topology'],
                'vnf_requirements': self.config['vnf_requirements'],
                'reward': self.config['reward'],
                'train': self.config['train'],
                'dimensions': self.config['dimensions']
            }
            
            # Edge-awareç¯å¢ƒï¼ˆä½¿ç”¨å®Œæ•´çš„4ç»´è¾¹ç‰¹å¾ï¼‰
            self.env_edge_aware = MultiVNFEmbeddingEnv(
                graph=self.graph.copy(),
                node_features=self.node_features.copy(),
                edge_features=self.edge_features.copy(),
                reward_config=reward_config,
                chain_length_range=chain_length_range,
                config=env_config.copy()
            )
            
            # Baselineç¯å¢ƒ
            self.env_baseline = MultiVNFEmbeddingEnv(
                graph=self.graph.copy(),
                node_features=self.node_features.copy(),
                edge_features=self.edge_features.copy(),
                reward_config=reward_config,
                chain_length_range=chain_length_range,
                config=env_config.copy()
            )
            # æ ‡è®°Baselineç¯å¢ƒ
            self.env_baseline.is_baseline_mode = True
            
            print(f"âœ… ç¯å¢ƒè®¾ç½®å®Œæˆ:")
            print(f"   - Edge-awareç¯å¢ƒ: 4ç»´è¾¹ç‰¹å¾")
            print(f"   - Baselineç¯å¢ƒ: 4ç»´ç¯å¢ƒç‰¹å¾ï¼Œæ™ºèƒ½ä½“æ„ŸçŸ¥2ç»´")
            
        except Exception as e:
            print(f"âŒ ç¯å¢ƒè®¾ç½®å¤±è´¥: {e}")
            raise
    
    def _setup_agents(self):
        """è®¾ç½®æ™ºèƒ½ä½“"""
        print("ğŸ¤– è®¾ç½®æ™ºèƒ½ä½“...")
        
        try:
            # âœ… ç¡®ä¿ç»´åº¦é…ç½®æ­£ç¡®
            state_dim = 8  # ç¯å¢ƒè¾“å‡ºçš„å›ºå®šç»´åº¦
            action_dim = len(self.graph.nodes())
            
            print(f"ğŸ“Š æ™ºèƒ½ä½“å‚æ•°:")
            print(f"   - çŠ¶æ€ç»´åº¦: {state_dim} (å›ºå®š)")
            print(f"   - åŠ¨ä½œç»´åº¦: {action_dim}")
            
            # Edge-awareæ™ºèƒ½ä½“
            self.agents_edge_aware = {}
            for agent_type in self.agent_types:
                agent_id = f"{agent_type}_edge_aware"
                edge_dim = self.config['gnn']['edge_aware']['edge_dim']  # 4ç»´
                
                self.agents_edge_aware[agent_type] = create_agent(
                    agent_type=agent_type,
                    agent_id=agent_id,
                    state_dim=state_dim,
                    action_dim=action_dim,
                    edge_dim=edge_dim,
                    config=self.config
                )
                print(f"âœ… {agent_id} åˆ›å»ºæˆåŠŸ (è¾¹ç‰¹å¾: {edge_dim}ç»´)")
            
            # Baselineæ™ºèƒ½ä½“
            self.agents_baseline = {}
            for agent_type in self.agent_types:
                agent_id = f"{agent_type}_baseline"
                edge_dim = self.config['gnn']['baseline']['edge_dim']  # 2ç»´
                
                self.agents_baseline[agent_type] = create_agent(
                    agent_type=agent_type,
                    agent_id=agent_id,
                    state_dim=state_dim,
                    action_dim=action_dim,
                    edge_dim=edge_dim,
                    config=self.config
                )
                print(f"âœ… {agent_id} åˆ›å»ºæˆåŠŸ (è¾¹ç‰¹å¾: {edge_dim}ç»´)")
            
            print(f"âœ… æ™ºèƒ½ä½“è®¾ç½®å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ æ™ºèƒ½ä½“è®¾ç½®å¤±è´¥: {e}")
            raise
    
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—è®°å½•"""
        print("ğŸ“Š è®¾ç½®æ—¥å¿—è®°å½•...")
        
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            self.loggers = {}
            
            for agent_type in self.agent_types:
                # Edge-awareæ—¥å¿—å™¨
                logger_id = f"{agent_type}_edge_aware"
                self.loggers[logger_id] = Logger(
                    log_dir=os.path.join(self.results_dir, f"{logger_id}_{timestamp}")
                )
                # Baselineæ—¥å¿—å™¨
                logger_id = f"{agent_type}_baseline"
                self.loggers[logger_id] = Logger(
                    log_dir=os.path.join(self.results_dir, f"{logger_id}_{timestamp}")
                )
            
            print(f"âœ… æ—¥å¿—è®°å½•è®¾ç½®å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ æ—¥å¿—è®¾ç½®å¤±è´¥: {e}")
            raise

    def _update_scenario(self, episode: int):
        """æ›´æ–°å½“å‰åœºæ™¯"""
        try:
            # ç¡®å®šå½“å‰åº”è¯¥æ˜¯å“ªä¸ªåœºæ™¯
            new_scenario = None
            if episode <= 25:
                new_scenario = "normal_operation"
            elif episode <= 50:
                new_scenario = "peak_congestion"
            elif episode <= 75:
                new_scenario = "failure_recovery"
            else:
                new_scenario = "extreme_pressure"
            
            # åªåœ¨åœºæ™¯çœŸæ­£æ”¹å˜æ—¶æ‰åº”ç”¨é…ç½®
            if new_scenario and new_scenario != self.current_scenario:
                print(f"\nğŸ¯ åœºæ™¯åˆ‡æ¢: {self.current_scenario} â†’ {new_scenario} (Episode {episode})")
                
                self.current_scenario = new_scenario
                self.scenario_start_episode = episode
                
                # è·å–åœºæ™¯é…ç½®
                scenario_config = get_scenario_config(episode)
                
                # åº”ç”¨åœºæ™¯é…ç½®åˆ°ç¯å¢ƒ
                print(f"ğŸ”§ åº”ç”¨åœºæ™¯é…ç½®...")
                self.env_edge_aware.apply_scenario_config(scenario_config)
                self.env_baseline.apply_scenario_config(scenario_config)
                
                self.last_applied_scenario = new_scenario
                return True
            
            return False
            
        except Exception as e:
            print(f"âš ï¸ åœºæ™¯æ›´æ–°å¤±è´¥: {e}")
            return False
    
    def train_single_episode(self, agent, env, agent_id: str) -> Dict[str, Any]:
        """è®­ç»ƒå•ä¸ªepisode"""
        try:
            state = env.reset()
            total_reward = 0.0
            step_count = 0
            success = False
            info = {}
            
            # é‡ç½®æ™ºèƒ½ä½“episodeç»Ÿè®¡
            if hasattr(agent, 'reset_episode_stats'):
                agent.reset_episode_stats()
            
            max_steps = getattr(env, 'max_episode_steps', 20)
            
            while step_count < max_steps:
                # è·å–æœ‰æ•ˆåŠ¨ä½œ
                valid_actions = env.get_valid_actions()
                if not valid_actions:
                    info = {'success': False, 'reason': 'no_valid_actions'}
                    break
                
                # é€‰æ‹©åŠ¨ä½œ
                try:
                    action = agent.select_action(state, valid_actions=valid_actions)
                    # âœ… ç¡®ä¿åŠ¨ä½œæ˜¯intç±»å‹
                    if isinstance(action, (list, np.ndarray)):
                        action = int(action[0]) if len(action) > 0 else valid_actions[0]
                    else:
                        action = int(action)
                    
                    # éªŒè¯åŠ¨ä½œæœ‰æ•ˆæ€§
                    if action not in valid_actions:
                        action = random.choice(valid_actions)
                        
                except Exception as e:
                    print(f"âš ï¸ åŠ¨ä½œé€‰æ‹©å¤±è´¥: {e}")
                    action = random.choice(valid_actions)
                
                # æ‰§è¡ŒåŠ¨ä½œ
                try:
                    next_state, reward, done, info = env.step(action)
                    
                    # å­˜å‚¨ç»éªŒ
                    agent.store_transition(state, action, reward, next_state, done)
                    
                    # æ›´æ–°çŠ¶æ€å’Œç»Ÿè®¡
                    state = next_state
                    total_reward += reward
                    step_count += 1
                    
                    # å­¦ä¹ æ›´æ–°
                    if hasattr(agent, 'should_update') and agent.should_update():
                        learning_info = agent.learn()
                    elif (hasattr(agent, 'replay_buffer') and 
                          len(getattr(agent, 'replay_buffer', [])) >= getattr(agent, 'batch_size', 32)):
                        learning_info = agent.learn()
                    
                    if done:
                        success = info.get('success', False)
                        break
                        
                except Exception as e:
                    print(f"âš ï¸ Stepæ‰§è¡Œå¤±è´¥: {e}")
                    break
            
            # æœ€åä¸€æ¬¡å­¦ä¹ æ›´æ–°
            try:
                if hasattr(agent, 'experiences') and len(getattr(agent, 'experiences', [])) > 0:
                    if hasattr(agent, 'should_update') and agent.should_update():
                        learning_info = agent.learn()
            except Exception as e:
                pass  # å¿½ç•¥å­¦ä¹ é”™è¯¯
            
            # è®¡ç®—episodeç»Ÿè®¡
            current_scenario_name = getattr(env, 'current_scenario_name', self.current_scenario)
            
            sar = 1.0 if success else 0.0
            splat = info.get('splat', info.get('avg_delay', float('inf'))) if success else float('inf')
            jitter = info.get('avg_jitter', 0.0) if success else 0.0
            loss = info.get('avg_loss', 0.0) if success else 0.0
            
            episode_stats = {
                'total_reward': total_reward,
                'steps': step_count,
                'success': success,
                'sar': sar,
                'splat': splat,
                'jitter': jitter,
                'loss': loss,
                'info': info,
                'scenario': current_scenario_name
            }
            
            return episode_stats
            
        except Exception as e:
            print(f"âŒ Episodeè®­ç»ƒå¤±è´¥: {e}")
            return {
                'total_reward': -10.0,
                'steps': 0,
                'success': False,
                'sar': 0.0,
                'splat': float('inf'),
                'jitter': 0.0,
                'loss': 0.0,
                'info': {'error': str(e)},
                'scenario': self.current_scenario
            }
    
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        print(f"\nğŸš€ å¼€å§‹å¤šæ™ºèƒ½ä½“æ¸è¿›å¼åœºæ™¯è®­ç»ƒ...")
        print(f"ç›®æ ‡episodes: {self.episodes}")
        
        # æ˜¾ç¤ºè®­ç»ƒè®¡åˆ’
        print_scenario_plan()
        
        all_results = {
            'edge_aware': {agent_type: {
                'rewards': [], 'sar': [], 'splat': [], 'success': [], 
                'jitter': [], 'loss': [], 'scenarios': []
            } for agent_type in self.agent_types},
            'baseline': {agent_type: {
                'rewards': [], 'sar': [], 'splat': [], 'success': [], 
                'jitter': [], 'loss': [], 'scenarios': []
            } for agent_type in self.agent_types}
        }
        
        # åˆå§‹åŒ–ç¬¬ä¸€ä¸ªåœºæ™¯
        print(f"ğŸ”§ åˆå§‹åŒ–ç¬¬ä¸€ä¸ªåœºæ™¯...")
        try:
            initial_scenario_config = get_scenario_config(1)
            self.env_edge_aware.apply_scenario_config(initial_scenario_config)
            self.env_baseline.apply_scenario_config(initial_scenario_config)
            print(f"âœ… åˆå§‹åœºæ™¯è®¾ç½®å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸ åˆå§‹åœºæ™¯è®¾ç½®å¤±è´¥: {e}")
        
        # ä¸»è®­ç»ƒå¾ªç¯
        for episode in range(1, self.episodes + 1):
            try:
                # æ£€æŸ¥å¹¶æ›´æ–°åœºæ™¯
                scenario_changed = self._update_scenario(episode)
                
                if episode % 25 == 0 or scenario_changed:
                    print(f"\nğŸ“ Episode {episode}/{self.episodes} - åœºæ™¯: {self.current_scenario}")
                
                # è®­ç»ƒEdge-awareæ™ºèƒ½ä½“
                for agent_type in self.agent_types:
                    try:
                        agent = self.agents_edge_aware[agent_type]
                        env = self.env_edge_aware
                        episode_stats = self.train_single_episode(agent, env, f"{agent_type}_edge_aware")
                        
                        # è®°å½•ç»“æœ
                        all_results['edge_aware'][agent_type]['rewards'].append(episode_stats['total_reward'])
                        all_results['edge_aware'][agent_type]['sar'].append(episode_stats['sar'])
                        all_results['edge_aware'][agent_type]['splat'].append(episode_stats['splat'])
                        all_results['edge_aware'][agent_type]['success'].append(episode_stats['success'])
                        all_results['edge_aware'][agent_type]['jitter'].append(episode_stats['jitter'])
                        all_results['edge_aware'][agent_type]['loss'].append(episode_stats['loss'])
                        all_results['edge_aware'][agent_type]['scenarios'].append(episode_stats['scenario'])
                        
                        # è®°å½•æ—¥å¿—
                        logger_id = f"{agent_type}_edge_aware"
                        if logger_id in self.loggers:
                            self.loggers[logger_id].log_episode(episode, episode_stats)
                    
                    except Exception as e:
                        print(f"âš ï¸ Edge-aware {agent_type} è®­ç»ƒå¤±è´¥: {e}")
                
                # è®­ç»ƒBaselineæ™ºèƒ½ä½“
                for agent_type in self.agent_types:
                    try:
                        agent = self.agents_baseline[agent_type]
                        env = self.env_baseline
                        episode_stats = self.train_single_episode(agent, env, f"{agent_type}_baseline")
                        
                        # è®°å½•ç»“æœ
                        all_results['baseline'][agent_type]['rewards'].append(episode_stats['total_reward'])
                        all_results['baseline'][agent_type]['sar'].append(episode_stats['sar'])
                        all_results['baseline'][agent_type]['splat'].append(episode_stats['splat'])
                        all_results['baseline'][agent_type]['success'].append(episode_stats['success'])
                        all_results['baseline'][agent_type]['jitter'].append(episode_stats['jitter'])
                        all_results['baseline'][agent_type]['loss'].append(episode_stats['loss'])
                        all_results['baseline'][agent_type]['scenarios'].append(episode_stats['scenario'])
                        
                        # è®°å½•æ—¥å¿—
                        logger_id = f"{agent_type}_baseline"
                        if logger_id in self.loggers:
                            self.loggers[logger_id].log_episode(episode, episode_stats)
                    
                    except Exception as e:
                        print(f"âš ï¸ Baseline {agent_type} è®­ç»ƒå¤±è´¥: {e}")
                
                # å®šæœŸæ‰“å°è¿›åº¦
                if episode % 25 == 0:
                    self._print_progress(episode, all_results)
                
                # å®šæœŸä¿å­˜æ¨¡å‹
                if episode % self.save_interval == 0:
                    self._save_models(episode)
                
                # å†…å­˜æ¸…ç†
                if episode % 10 == 0:
                    self._cleanup_memory()
                    
            except Exception as e:
                print(f"âŒ Episode {episode} è®­ç»ƒå¤±è´¥: {e}")
                continue
        
        # æœ€ç»ˆåˆ†æ
        try:
            self._final_analysis(all_results)
        except Exception as e:
            print(f"âš ï¸ æœ€ç»ˆåˆ†æå¤±è´¥: {e}")
        
        print(f"\nğŸ‰ æ¸è¿›å¼åœºæ™¯è®­ç»ƒå®Œæˆ!")
        return all_results
    
    def _print_progress(self, episode: int, results: Dict):
        """æ‰“å°è®­ç»ƒè¿›åº¦"""
        try:
            print(f"\nğŸ“Š Episode {episode} æ€§èƒ½ç»Ÿè®¡ (åœºæ™¯: {self.current_scenario}):")
            window = 25
            start_idx = max(0, episode - window)
            
            for variant in ['edge_aware', 'baseline']:
                print(f"\n{variant.upper()}:")
                for agent_type in self.agent_types:
                    if agent_type in results[variant]:
                        recent_sar = np.mean(results[variant][agent_type]['sar'][start_idx:])
                        recent_splat_values = [s for s in results[variant][agent_type]['splat'][start_idx:] 
                                             if s != float('inf')]
                        recent_splat = np.mean(recent_splat_values) if recent_splat_values else float('inf')
                        recent_reward = np.mean(results[variant][agent_type]['rewards'][start_idx:])
                        
                        print(f"  {agent_type.upper()}:")
                        print(f"    SAR: {recent_sar:.3f}")
                        print(f"    SPLat: {recent_splat:.2f}")
                        print(f"    Reward: {recent_reward:.1f}")
                        
        except Exception as e:
            print(f"âš ï¸ è¿›åº¦æ‰“å°å¤±è´¥: {e}")
    
    def _save_models(self, episode: int):
        """ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹"""
        try:
            checkpoint_dir = os.path.join(self.results_dir, "checkpoints", f"episode_{episode}")
            os.makedirs(checkpoint_dir, exist_ok=True)
            
            # ä¿å­˜Edge-awareæ™ºèƒ½ä½“
            for agent_type, agent in self.agents_edge_aware.items():
                filepath = os.path.join(checkpoint_dir, f"{agent_type}_edge_aware.pth")
                if hasattr(agent, 'save_checkpoint'):
                    agent.save_checkpoint(filepath)
            
            # ä¿å­˜Baselineæ™ºèƒ½ä½“
            for agent_type, agent in self.agents_baseline.items():
                filepath = os.path.join(checkpoint_dir, f"{agent_type}_baseline.pth")
                if hasattr(agent, 'save_checkpoint'):
                    agent.save_checkpoint(filepath)
                    
            print(f"ğŸ’¾ æ¨¡å‹æ£€æŸ¥ç‚¹å·²ä¿å­˜: episode_{episode}")
            
        except Exception as e:
            print(f"âš ï¸ æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
    
    def _cleanup_memory(self):
        """æ¸…ç†å†…å­˜"""
        try:
            # æ¸…ç†æ™ºèƒ½ä½“å†…å­˜
            for agents in [self.agents_edge_aware, self.agents_baseline]:
                for agent in agents.values():
                    if hasattr(agent, 'cleanup_memory'):
                        agent.cleanup_memory()
            
            # æ¸…ç†GPUå†…å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                
        except Exception as e:
            print(f"âš ï¸ å†…å­˜æ¸…ç†å¤±è´¥: {e}")
    
    def _final_analysis(self, results: Dict):
        """æœ€ç»ˆæ€§èƒ½åˆ†æ"""
        print(f"\nğŸ¯ æœ€ç»ˆæ€§èƒ½åˆ†æ:")
        print(f"{'='*70}")
        
        try:
            # æŒ‰åœºæ™¯åˆ†ç»„åˆ†æ
            scenario_data = []
            
            for scenario_name in ['normal_operation', 'peak_congestion', 'failure_recovery', 'extreme_pressure']:
                print(f"\nğŸ“‹ {scenario_name}:")
                
                for variant in ['edge_aware', 'baseline']:
                    print(f"\n  {variant.upper()}:")
                    for agent_type in self.agent_types:
                        if agent_type in results[variant]:
                            # è·å–è¯¥åœºæ™¯çš„æ•°æ®
                            scenario_episodes = []
                            for i, ep_scenario in enumerate(results[variant][agent_type]['scenarios']):
                                if ep_scenario == scenario_name:
                                    scenario_episodes.append(i)
                            
                            if scenario_episodes:
                                # è®¡ç®—å¹³å‡æ€§èƒ½
                                recent_episodes = scenario_episodes[-10:] if len(scenario_episodes) >= 10 else scenario_episodes
                                
                                scenario_sar = np.mean([results[variant][agent_type]['sar'][i] for i in recent_episodes])
                                scenario_splat_values = [results[variant][agent_type]['splat'][i] for i in recent_episodes 
                                                       if results[variant][agent_type]['splat'][i] != float('inf')]
                                scenario_splat = np.mean(scenario_splat_values) if scenario_splat_values else float('inf')
                                scenario_reward = np.mean([results[variant][agent_type]['rewards'][i] for i in recent_episodes])
                                
                                print(f"    {agent_type.upper()}:")
                                print(f"      SAR: {scenario_sar:.3f}")
                                print(f"      SPLat: {scenario_splat:.2f}")
                                print(f"      Reward: {scenario_reward:.1f}")
                                
                                scenario_data.append({
                                    'Scenario': scenario_name,
                                    'Variant': variant,
                                    'Algorithm': agent_type.upper(),
                                    'SAR': scenario_sar,
                                    'SPLat': scenario_splat,
                                    'Reward': scenario_reward
                                })
            
            # ä¿å­˜ç»“æœ
            df_scenarios = pd.DataFrame(scenario_data)
            results_csv_path = os.path.join(self.results_dir, 'scenario_results.csv')
            df_scenarios.to_csv(results_csv_path, index=False)
            
            print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜: {results_csv_path}")
            
        except Exception as e:
            print(f"âš ï¸ æœ€ç»ˆåˆ†æå¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='VNFåµŒå…¥å¤šæ™ºèƒ½ä½“è®­ç»ƒ (ä¿®å¤ç‰ˆ)')
    parser.add_argument('--config', type=str, default='config.yaml', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--episodes', type=int, default=None, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--test', action='store_true', help='è¿è¡Œæµ‹è¯•æ¨¡å¼')
    args = parser.parse_args()
    
    try:
        # éªŒè¯é…ç½®æ–‡ä»¶
        if not os.path.exists(args.config):
            print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
            return
        
        # è¿è¡Œæµ‹è¯•æ¨¡å¼
        if args.test:
            print("ğŸ§ª è¿è¡Œæµ‹è¯•æ¨¡å¼...")
            # è¿™é‡Œå¯ä»¥æ·»åŠ æµ‹è¯•ä»£ç 
            return
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = MultiAgentTrainer(config_path=args.config)
        
        # è®¾ç½®è®­ç»ƒè½®æ•°
        if args.episodes:
            trainer.episodes = args.episodes
            trainer.config['train']['episodes'] = args.episodes
        
        # å¼€å§‹è®­ç»ƒ
        results = trainer.train()
        
        print(f"\nâœ… è®­ç»ƒä»»åŠ¡å®Œæˆ!")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {trainer.results_dir}")
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()