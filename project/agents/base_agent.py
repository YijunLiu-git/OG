# agents/base_agent.py - ä¿®å¤ç‰ˆ

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from abc import ABC, abstractmethod
from typing import Union, List, Dict, Any, Optional
from torch_geometric.data import Data, Batch

from models.gnn_encoder import GNNEncoder

class BaseAgent(ABC):
    """
    å¤šæ™ºèƒ½ä½“VNFåµŒå…¥ç³»ç»Ÿçš„åŸºç¡€æ™ºèƒ½ä½“ç±» - ä¿®å¤ç‰ˆ
    
    ä¸»è¦ä¿®å¤ï¼š
    1. âœ… ç»Ÿä¸€åŠ¨ä½œé€‰æ‹©æ¥å£ï¼šåªè¿”å›int
    2. âœ… ç¡®ä¿çŠ¶æ€ç»´åº¦ä¸€è‡´æ€§
    3. âœ… æ”¹è¿›é”™è¯¯å¤„ç†æœºåˆ¶
    4. âœ… æ·»åŠ å†…å­˜ç®¡ç†
    """
    
    def __init__(self, 
                 agent_id: str,
                 state_dim: int, 
                 action_dim: int, 
                 edge_dim: int,
                 config: Dict[str, Any]):
        
        self.agent_id = agent_id
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.edge_dim = edge_dim
        self.config = config
        
        # è®¾å¤‡é…ç½®
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ¤– Agent {agent_id} ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # âœ… ä¿®å¤ï¼šç¡®ä¿ç»´åº¦é…ç½®æ­£ç¡®
        assert state_dim == 8, f"çŠ¶æ€ç»´åº¦å¿…é¡»ä¸º8ï¼Œå½“å‰ä¸º{state_dim}"
        assert edge_dim in [2, 4], f"è¾¹ç»´åº¦å¿…é¡»ä¸º2æˆ–4ï¼Œå½“å‰ä¸º{edge_dim}"
        
        # é€‰æ‹© GNN é…ç½®
        gnn_config = config.get("gnn", {}).get("edge_aware" if "edge_aware" in agent_id else "baseline", {})
        self.hidden_dim = gnn_config.get("hidden_dim", 128)
        self.output_dim = gnn_config.get("output_dim", 256)
        self.num_layers = gnn_config.get("layers", 4)
        
        # è®­ç»ƒé…ç½®
        self.learning_rate = config.get("train", {}).get("lr", 0.001)
        self.gamma = config.get("train", {}).get("gamma", 0.99)
        self.batch_size = config.get("train", {}).get("batch_size", 32)
        
        # æ¢ç´¢é…ç½®
        self.epsilon = config.get("train", {}).get("epsilon_start", 1.0)
        self.epsilon_decay = config.get("train", {}).get("epsilon_decay", 0.995)
        self.epsilon_min = config.get("train", {}).get("epsilon_min", 0.01)
        
        # âœ… ä¿®å¤ï¼šå›¾ç¥ç»ç½‘ç»œç¼–ç å™¨ï¼Œç¡®ä¿ç»´åº¦æ­£ç¡®
        try:
            self.gnn_encoder = GNNEncoder(
                node_dim=state_dim,  # ç¡®ä¿æ˜¯8ç»´
                edge_dim=edge_dim,   # 2ç»´æˆ–4ç»´
                hidden_dim=self.hidden_dim,
                output_dim=self.output_dim,
                num_layers=self.num_layers
            ).to(self.device)
            print(f"âœ… GNNç¼–ç å™¨åˆå§‹åŒ–æˆåŠŸ: {state_dim}ç»´èŠ‚ç‚¹ -> {self.output_dim}ç»´è¾“å‡º")
        except Exception as e:
            print(f"âŒ GNNç¼–ç å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
        
        # ç­–ç•¥ç½‘ç»œï¼ˆå­ç±»å®ç°å…·ä½“ç»“æ„ï¼‰
        self.policy_network = None
        self.target_network = None  # DQNç³»åˆ—ä½¿ç”¨
        self.optimizer = None
        
        # è®­ç»ƒçŠ¶æ€
        self.training_step = 0
        self.episode_count = 0
        self.is_training = True
        
        # âœ… ä¿®å¤ï¼šæ”¹è¿›ç»Ÿè®¡ä¿¡æ¯ç»“æ„
        self.stats = {
            "total_reward": 0.0,
            "episodes": 0,
            "steps": 0,
            "losses": [],
            "q_values": [],
            "actions_taken": {},
            "errors": [],  # æ–°å¢é”™è¯¯è®°å½•
            "memory_usage": []  # æ–°å¢å†…å­˜ä½¿ç”¨è®°å½•
        }
        
        # å¤šæ™ºèƒ½ä½“åè°ƒï¼ˆé¢„ç•™ï¼‰
        self.other_agents = {}
        self.communication_enabled = False
    
    def process_state(self, state: Union[Data, Dict, np.ndarray]) -> torch.Tensor:
        """
        å¤„ç†çŠ¶æ€è¾“å…¥ - ä¿®å¤ç‰ˆ
        
        Args:
            state: å¯ä»¥æ˜¯PyG Dataå¯¹è±¡ã€å­—å…¸æˆ–numpyæ•°ç»„
            
        Returns:
            processed_state: å¤„ç†åçš„çŠ¶æ€tensor [1, output_dim]
        """
        try:
            self.gnn_encoder.eval()
            
            with torch.no_grad():
                if isinstance(state, Data):
                    # âœ… ä¿®å¤ï¼šç¡®ä¿æ•°æ®åœ¨æ­£ç¡®è®¾å¤‡ä¸Šï¼Œé¿å…å†…å­˜æ³„æ¼
                    state = state.to(self.device)
                    # éªŒè¯ç»´åº¦
                    if state.x.size(1) != self.state_dim:
                        raise ValueError(f"çŠ¶æ€ç»´åº¦ä¸åŒ¹é…: æœŸæœ›{self.state_dim}ï¼Œå®é™…{state.x.size(1)}")
                    encoded_state = self.gnn_encoder(state)
                    
                elif isinstance(state, dict) and 'graph_data' in state:
                    graph_data = state['graph_data'].to(self.device)
                    if graph_data.x.size(1) != self.state_dim:
                        raise ValueError(f"çŠ¶æ€ç»´åº¦ä¸åŒ¹é…: æœŸæœ›{self.state_dim}ï¼Œå®é™…{graph_data.x.size(1)}")
                    encoded_state = self.gnn_encoder(graph_data)
                    
                elif isinstance(state, (np.ndarray, torch.Tensor)):
                    if isinstance(state, np.ndarray):
                        state = torch.tensor(state, dtype=torch.float32)
                    if state.dim() == 1:
                        state = state.unsqueeze(0)
                    # âœ… ä¿®å¤ï¼šç¡®ä¿ç»´åº¦æ­£ç¡®
                    if state.size(1) != self.output_dim:
                        print(f"âš ï¸ çŠ¶æ€ç»´åº¦({state.size(1)})ä¸æ˜¯è¾“å‡ºç»´åº¦({self.output_dim})ï¼Œå‡è®¾ä¸ºå·²ç¼–ç çŠ¶æ€")
                    encoded_state = state.to(self.device)
                    
                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„çŠ¶æ€æ ¼å¼: {type(state)}")
            
            if self.is_training:
                self.gnn_encoder.train()
            
            # âœ… ä¿®å¤ï¼šç¡®ä¿è¾“å‡ºç»´åº¦æ­£ç¡®
            if encoded_state.size(-1) != self.output_dim:
                print(f"âš ï¸ ç¼–ç åçŠ¶æ€ç»´åº¦({encoded_state.size(-1)})ä¸åŒ¹é…æœŸæœ›ç»´åº¦({self.output_dim})")
                
            return encoded_state
            
        except Exception as e:
            print(f"âŒ çŠ¶æ€å¤„ç†å¤±è´¥: {e}")
            self.stats["errors"].append(f"çŠ¶æ€å¤„ç†é”™è¯¯: {e}")
            # è¿”å›é»˜è®¤çŠ¶æ€
            return torch.zeros(1, self.output_dim, device=self.device)
    
    def update_target_network(self, tau: float = None):
        """æ›´æ–°ç›®æ ‡ç½‘ç»œï¼ˆç”¨äºDQNç³»åˆ—ç®—æ³•ï¼‰"""
        if self.target_network is None:
            return
            
        try:
            if tau is None:
                self.target_network.load_state_dict(self.policy_network.state_dict())
            else:
                for target_param, policy_param in zip(
                    self.target_network.parameters(), 
                    self.policy_network.parameters()
                ):
                    target_param.data.copy_(
                        tau * policy_param.data + (1 - tau) * target_param.data
                    )
        except Exception as e:
            print(f"âŒ ç›®æ ‡ç½‘ç»œæ›´æ–°å¤±è´¥: {e}")
            self.stats["errors"].append(f"ç›®æ ‡ç½‘ç»œæ›´æ–°é”™è¯¯: {e}")
    
    def decay_epsilon(self):
        """æ›´æ–°æ¢ç´¢ç‡"""
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
    
    def get_valid_actions(self, state: Union[Data, Dict], **kwargs) -> List[int]:
        """è·å–å½“å‰çŠ¶æ€ä¸‹çš„æœ‰æ•ˆåŠ¨ä½œ"""
        # âœ… ä¿®å¤ï¼šæ·»åŠ æ›´å¥å£®çš„æœ‰æ•ˆåŠ¨ä½œæ£€æŸ¥
        try:
            available_nodes = kwargs.get('available_nodes', list(range(self.action_dim)))
            resource_constraints = kwargs.get('resource_constraints', {})
            
            valid_actions = []
            for action in available_nodes:
                if 0 <= action < self.action_dim:
                    valid_actions.append(action)
            
            # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªæœ‰æ•ˆåŠ¨ä½œ
            if not valid_actions:
                valid_actions = [0]
                print("âš ï¸ æ²¡æœ‰æœ‰æ•ˆåŠ¨ä½œï¼Œä½¿ç”¨é»˜è®¤åŠ¨ä½œ0")
            
            return valid_actions
            
        except Exception as e:
            print(f"âŒ è·å–æœ‰æ•ˆåŠ¨ä½œå¤±è´¥: {e}")
            return list(range(min(5, self.action_dim)))  # è¿”å›å‰5ä¸ªåŠ¨ä½œä½œä¸ºå¤‡é€‰
    
    def mask_invalid_actions(self, q_values: torch.Tensor, valid_actions: List[int]) -> torch.Tensor:
        """å±è”½æ— æ•ˆåŠ¨ä½œçš„Qå€¼"""
        try:
            masked_q_values = q_values.clone()
            invalid_actions = [a for a in range(self.action_dim) if a not in valid_actions]
            
            if invalid_actions:
                masked_q_values[:, invalid_actions] = -float('inf')
            
            return masked_q_values
            
        except Exception as e:
            print(f"âŒ åŠ¨ä½œå±è”½å¤±è´¥: {e}")
            return q_values  # è¿”å›åŸå§‹Qå€¼
    
    def update_stats(self, reward: float, action: int, loss: float = None, q_values: torch.Tensor = None):
        """æ›´æ–°æ™ºèƒ½ä½“ç»Ÿè®¡ä¿¡æ¯"""
        try:
            self.stats["total_reward"] += float(reward)
            self.stats["steps"] += 1
            
            if loss is not None:
                self.stats["losses"].append(float(loss))
            
            if q_values is not None:
                self.stats["q_values"].append(float(q_values.mean().item()))
            
            if action not in self.stats["actions_taken"]:
                self.stats["actions_taken"][action] = 0
            self.stats["actions_taken"][action] += 1
            
            # âœ… æ–°å¢ï¼šè®°å½•å†…å­˜ä½¿ç”¨
            if torch.cuda.is_available():
                memory_used = torch.cuda.memory_allocated(self.device) / 1024**2  # MB
                self.stats["memory_usage"].append(memory_used)
                
        except Exception as e:
            print(f"âŒ ç»Ÿè®¡æ›´æ–°å¤±è´¥: {e}")
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–æ™ºèƒ½ä½“ç»Ÿè®¡ä¿¡æ¯"""
        try:
            stats = self.stats.copy()
            
            if stats["episodes"] > 0:
                stats["avg_reward"] = stats["total_reward"] / stats["episodes"]
            else:
                stats["avg_reward"] = 0.0
                
            if stats["losses"]:
                stats["avg_loss"] = np.mean(stats["losses"][-100:])
                
            if stats["q_values"]:
                stats["avg_q_value"] = np.mean(stats["q_values"][-100:])
                
            if stats["memory_usage"]:
                stats["avg_memory_mb"] = np.mean(stats["memory_usage"][-50:])
                stats["max_memory_mb"] = np.max(stats["memory_usage"])
                
            stats["epsilon"] = self.epsilon
            stats["error_count"] = len(stats["errors"])
            
            return stats
            
        except Exception as e:
            print(f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {"error": str(e)}
    
    def reset_episode_stats(self):
        """é‡ç½®episodeç»Ÿè®¡"""
        self.stats["total_reward"] = 0.0
        self.stats["episodes"] += 1
    
    def cleanup_memory(self):
        """âœ… æ–°å¢ï¼šæ¸…ç†å†…å­˜"""
        try:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                
            # æ¸…ç†æ—§çš„ç»Ÿè®¡æ•°æ®
            if len(self.stats["losses"]) > 1000:
                self.stats["losses"] = self.stats["losses"][-500:]
            if len(self.stats["q_values"]) > 1000:
                self.stats["q_values"] = self.stats["q_values"][-500:]
            if len(self.stats["memory_usage"]) > 200:
                self.stats["memory_usage"] = self.stats["memory_usage"][-100:]
                
        except Exception as e:
            print(f"âŒ å†…å­˜æ¸…ç†å¤±è´¥: {e}")
    
    def save_checkpoint(self, filepath: str):
        """ä¿å­˜æ™ºèƒ½ä½“æ£€æŸ¥ç‚¹"""
        try:
            checkpoint = {
                'agent_id': self.agent_id,
                'training_step': self.training_step,
                'episode_count': self.episode_count,
                'epsilon': self.epsilon,
                'stats': self.stats,
                'gnn_encoder_state': self.gnn_encoder.state_dict(),
                'config': self.config  # âœ… æ–°å¢ï¼šä¿å­˜é…ç½®
            }
            
            if self.policy_network is not None:
                checkpoint['policy_network_state'] = self.policy_network.state_dict()
                
            if self.target_network is not None:
                checkpoint['target_network_state'] = self.target_network.state_dict()
                
            if self.optimizer is not None:
                checkpoint['optimizer_state'] = self.optimizer.state_dict()
            
            torch.save(checkpoint, filepath)
            print(f"ğŸ’¾ Agent {self.agent_id} æ£€æŸ¥ç‚¹å·²ä¿å­˜: {filepath}")
            
        except Exception as e:
            print(f"âŒ æ£€æŸ¥ç‚¹ä¿å­˜å¤±è´¥: {e}")
            self.stats["errors"].append(f"æ£€æŸ¥ç‚¹ä¿å­˜é”™è¯¯: {e}")
    
    def load_checkpoint(self, filepath: str):
        """åŠ è½½æ™ºèƒ½ä½“æ£€æŸ¥ç‚¹"""
        try:
            checkpoint = torch.load(filepath, map_location=self.device)
            
            self.training_step = checkpoint['training_step']
            self.episode_count = checkpoint['episode_count']
            self.epsilon = checkpoint['epsilon']
            self.stats = checkpoint['stats']
            
            self.gnn_encoder.load_state_dict(checkpoint['gnn_encoder_state'])
            
            if 'policy_network_state' in checkpoint and self.policy_network is not None:
                self.policy_network.load_state_dict(checkpoint['policy_network_state'])
                
            if 'target_network_state' in checkpoint and self.target_network is not None:
                self.target_network.load_state_dict(checkpoint['target_network_state'])
                
            if 'optimizer_state' in checkpoint and self.optimizer is not None:
                self.optimizer.load_state_dict(checkpoint['optimizer_state'])
            
            print(f"ğŸ“‚ Agent {self.agent_id} æ£€æŸ¥ç‚¹å·²åŠ è½½: {filepath}")
            
        except Exception as e:
            print(f"âŒ æ£€æŸ¥ç‚¹åŠ è½½å¤±è´¥: {e}")
            self.stats["errors"].append(f"æ£€æŸ¥ç‚¹åŠ è½½é”™è¯¯: {e}")
    
    def set_training_mode(self, training: bool = True):
        """è®¾ç½®è®­ç»ƒ/è¯„ä¼°æ¨¡å¼"""
        self.is_training = training
        
        try:
            if training:
                self.gnn_encoder.train()
                if self.policy_network is not None:
                    self.policy_network.train()
            else:
                self.gnn_encoder.eval()
                if self.policy_network is not None:
                    self.policy_network.eval()
        except Exception as e:
            print(f"âŒ è®¾ç½®è®­ç»ƒæ¨¡å¼å¤±è´¥: {e}")
    
    # âœ… ä¿®å¤ï¼šç»Ÿä¸€åŠ¨ä½œé€‰æ‹©æ¥å£
    @abstractmethod
    def select_action(self, state: Union[Data, Dict], **kwargs) -> int:
        """
        é€‰æ‹©åŠ¨ä½œ - ç»Ÿä¸€æ¥å£
        
        Args:
            state: å½“å‰çŠ¶æ€
            **kwargs: é¢å¤–å‚æ•°
            
        Returns:
            action: é€‰æ‹©çš„å•ä¸ªåŠ¨ä½œï¼ˆintï¼‰
        """
        pass
    
    @abstractmethod
    def store_transition(self, state, action, reward, next_state, done, **kwargs):
        pass
    
    @abstractmethod
    def learn(self) -> Dict[str, float]:
        pass

# âœ… ä¿®å¤ï¼šæ”¹è¿›å·¥å‚å‡½æ•°
def create_agent(agent_type: str, agent_id: str, state_dim: int, action_dim: int, 
                edge_dim: int, config: Dict[str, Any]) -> BaseAgent:
    """
    å·¥å‚å‡½æ•°ï¼šåˆ›å»ºæŒ‡å®šç±»å‹çš„æ™ºèƒ½ä½“ - ä¿®å¤ç‰ˆ
    """
    # âœ… éªŒè¯è¾“å…¥å‚æ•°
    assert state_dim == 8, f"çŠ¶æ€ç»´åº¦å¿…é¡»ä¸º8ï¼Œå½“å‰ä¸º{state_dim}"
    assert edge_dim in [2, 4], f"è¾¹ç»´åº¦å¿…é¡»ä¸º2æˆ–4ï¼Œå½“å‰ä¸º{edge_dim}"
    assert action_dim > 0, f"åŠ¨ä½œç»´åº¦å¿…é¡»å¤§äº0ï¼Œå½“å‰ä¸º{action_dim}"
    
    try:
        if agent_type.lower() == 'ddqn':
            from agents.multi_ddqn_agent import MultiDDQNAgent
            return MultiDDQNAgent(agent_id, state_dim, action_dim, edge_dim, config)
        elif agent_type.lower() == 'dqn':
            from agents.multi_dqn_agent import MultiDQNAgent
            return MultiDQNAgent(agent_id, state_dim, action_dim, edge_dim, config)
        elif agent_type.lower() == 'ppo':
            from agents.multi_ppo_agent import MultiPPOAgent
            return MultiPPOAgent(agent_id, state_dim, action_dim, edge_dim, config)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ™ºèƒ½ä½“ç±»å‹: {agent_type}")
            
    except Exception as e:
        print(f"âŒ æ™ºèƒ½ä½“åˆ›å»ºå¤±è´¥: {e}")
        raise

def test_base_agent():
    """æµ‹è¯•BaseAgentåŸºç¡€åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•ä¿®å¤åçš„BaseAgent...")
    
    config = {
        "gnn": {
            "edge_aware": {"hidden_dim": 64, "output_dim": 128, "layers": 4},
            "baseline": {"hidden_dim": 64, "output_dim": 128, "layers": 4}
        },
        "train": {"lr": 0.001, "gamma": 0.99, "batch_size": 16}
    }
    
    class TestAgent(BaseAgent):
        def __init__(self, agent_id, state_dim, action_dim, edge_dim, config):
            super().__init__(agent_id, state_dim, action_dim, edge_dim, config)
            
        def select_action(self, state, **kwargs) -> int:  # âœ… ä¿®å¤ï¼šæ˜ç¡®è¿”å›int
            valid_actions = self.get_valid_actions(state, **kwargs)
            return np.random.choice(valid_actions)
            
        def store_transition(self, state, action, reward, next_state, done, **kwargs):
            pass
            
        def learn(self):
            return {"loss": 0.1}
    
    try:
        agent = TestAgent("test_agent", state_dim=8, action_dim=10, edge_dim=4, config=config)
        
        test_state = torch.randn(1, 128)  # å‡è®¾å·²ç¼–ç çŠ¶æ€
        processed_state = agent.process_state(test_state)
        
        print(f"âœ… çŠ¶æ€å¤„ç†æµ‹è¯•: {processed_state.shape}")
        print(f"âœ… æ™ºèƒ½ä½“åˆ›å»ºæˆåŠŸ: {agent.agent_id}")
        print(f"âœ… è®¾å¤‡é…ç½®: {agent.device}")
        
        # æµ‹è¯•åŠ¨ä½œé€‰æ‹©
        action = agent.select_action(test_state)
        assert isinstance(action, (int, np.integer)), f"åŠ¨ä½œåº”è¯¥æ˜¯intç±»å‹ï¼Œå®é™…æ˜¯{type(action)}"
        print(f"âœ… åŠ¨ä½œé€‰æ‹©æµ‹è¯•: {action} (ç±»å‹: {type(action)})")
        
        agent.update_stats(reward=1.0, action=action, loss=0.1)
        stats = agent.get_stats()
        print(f"âœ… ç»Ÿè®¡åŠŸèƒ½æµ‹è¯•: {stats['total_reward']}")
        
        # æµ‹è¯•å†…å­˜æ¸…ç†
        agent.cleanup_memory()
        print("âœ… å†…å­˜æ¸…ç†æµ‹è¯•é€šè¿‡")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        raise

if __name__ == "__main__":
    test_base_agent()