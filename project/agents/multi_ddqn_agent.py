# agents/multi_ddqn_agent.py - ä¿®å¤ç‰ˆ

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Union, List, Dict, Any
from torch_geometric.data import Data

from agents.base_agent import BaseAgent
from utils.replay_buffer import PrioritizedReplayBuffer

class DDQNNetwork(nn.Module):
    """
    åŒæ·±åº¦Qç½‘ç»œ - ä¿®å¤ç‰ˆï¼ˆè§£å†³BatchNormé—®é¢˜ï¼‰
    """
    
    def __init__(self, input_dim: int, action_dim: int, hidden_dim: int = 512):
        super(DDQNNetwork, self).__init__()
        
        self.input_dim = input_dim
        self.action_dim = action_dim
        
        # âœ… ä¿®å¤ï¼šç§»é™¤BatchNormï¼Œä½¿ç”¨LayerNormæ›¿ä»£
        self.q_network = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.LayerNorm(hidden_dim),  # ä½¿ç”¨LayerNormæ›¿ä»£BatchNorm
            nn.Dropout(0.2),
            
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.LayerNorm(hidden_dim // 2),
            nn.Dropout(0.1),
            
            nn.Linear(hidden_dim // 2, hidden_dim // 4),
            nn.ReLU(),
            nn.Linear(hidden_dim // 4, action_dim)
        )
        
        # ç½‘ç»œåˆå§‹åŒ–
        self._initialize_weights()
    
    def _initialize_weights(self):
        """Xavieråˆå§‹åŒ–"""
        for layer in self.q_network:
            if isinstance(layer, nn.Linear):
                nn.init.xavier_uniform_(layer.weight)
                nn.init.constant_(layer.bias, 0.01)
    
    def forward(self, state_embedding: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­ - ä¿®å¤ç‰ˆ
        
        Args:
            state_embedding: GNNç¼–ç åçš„çŠ¶æ€ [batch_size, input_dim]
            
        Returns:
            q_values: Qå€¼ [batch_size, action_dim]
        """
        # âœ… ä¿®å¤ï¼šLayerNormå¯ä»¥å¤„ç†ä»»æ„æ‰¹æ¬¡å¤§å°
        return self.q_network(state_embedding)


class MultiDDQNAgent(BaseAgent):
    """
    å¤šæ™ºèƒ½ä½“åŒæ·±åº¦Qå­¦ä¹ æ™ºèƒ½ä½“ - ä¿®å¤ç‰ˆ
    """
    
    def __init__(self, agent_id: str, state_dim: int, action_dim: int, edge_dim: int, config: Dict[str, Any]):
        super().__init__(agent_id, state_dim, action_dim, edge_dim, config)
        
        # DDQNç‰¹å®šé…ç½®
        self.target_update_freq = config.get("train", {}).get("target_update", 100)
        self.double_q = True  # å¯ç”¨åŒQå­¦ä¹ 
        
        # âœ… ä¿®å¤ï¼šç¡®ä¿ç½‘ç»œè¾“å…¥ç»´åº¦æ­£ç¡®
        network_input_dim = self.output_dim  # GNNEncoderçš„è¾“å‡ºç»´åº¦ (åº”è¯¥æ˜¯256)
        
        try:
            # ç­–ç•¥ç½‘ç»œï¼ˆä¸»ç½‘ç»œï¼‰
            self.policy_network = DDQNNetwork(
                input_dim=network_input_dim,
                action_dim=action_dim,
                hidden_dim=config.get("network", {}).get("hidden_dim", 512)
            ).to(self.device)
            
            # ç›®æ ‡ç½‘ç»œ
            self.target_network = DDQNNetwork(
                input_dim=network_input_dim,
                action_dim=action_dim,
                hidden_dim=config.get("network", {}).get("hidden_dim", 512)
            ).to(self.device)
            
            # åˆå§‹åŒ–ç›®æ ‡ç½‘ç»œ
            self.target_network.load_state_dict(self.policy_network.state_dict())
            
            print(f"âœ… DDQNç½‘ç»œåˆå§‹åŒ–æˆåŠŸ: {network_input_dim} -> {action_dim}")
            
        except Exception as e:
            print(f"âŒ DDQNç½‘ç»œåˆå§‹åŒ–å¤±è´¥: {e}")
            raise
        
        # ä¼˜åŒ–å™¨
        self.optimizer = torch.optim.Adam(
            list(self.gnn_encoder.parameters()) + list(self.policy_network.parameters()),
            lr=self.learning_rate,
            weight_decay=1e-5
        )
        
        # ä¼˜å…ˆçº§ç»éªŒå›æ”¾
        buffer_size = config.get("train", {}).get("buffer_size", 10000)
        self.replay_buffer = PrioritizedReplayBuffer(
            capacity=buffer_size,
            alpha=0.6,  # ä¼˜å…ˆçº§æŒ‡æ•°
            beta=0.4    # é‡è¦æ€§é‡‡æ ·æŒ‡æ•°
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.lr_scheduler = torch.optim.lr_scheduler.StepLR(
            self.optimizer, step_size=1000, gamma=0.95
        )
        
        print(f"ğŸš€ DDQN Agent {agent_id} åˆå§‹åŒ–å®Œæˆ")
    
    def select_action(self, state: Union[Data, Dict], valid_actions: List[int] = None, **kwargs) -> int:
        """
        é€‰æ‹©åŠ¨ä½œ - ä¿®å¤ç‰ˆï¼šç¡®ä¿è¿”å›å•ä¸ªint
        
        Args:
            state: å½“å‰çŠ¶æ€ï¼ˆå›¾æ•°æ®æˆ–ç¼–ç åçŠ¶æ€ï¼‰
            valid_actions: æœ‰æ•ˆåŠ¨ä½œåˆ—è¡¨
            **kwargs: é¢å¤–å‚æ•°
            
        Returns:
            action: é€‰æ‹©çš„å•ä¸ªåŠ¨ä½œ (int)
        """
        try:
            # å¤„ç†çŠ¶æ€
            state_embedding = self.process_state(state)
            
            # âœ… ä¿®å¤ï¼šç¡®ä¿çŠ¶æ€ç»´åº¦æ­£ç¡®
            if state_embedding.size(-1) != self.output_dim:
                print(f"âš ï¸ çŠ¶æ€ç»´åº¦ä¸åŒ¹é…: æœŸæœ›{self.output_dim}ï¼Œå®é™…{state_embedding.size(-1)}")
                # å¦‚æœç»´åº¦ä¸å¯¹ï¼Œè¿›è¡Œé€‚é…
                if state_embedding.size(-1) < self.output_dim:
                    # å¡«å……é›¶
                    padding = torch.zeros(state_embedding.size(0), 
                                        self.output_dim - state_embedding.size(-1), 
                                        device=self.device)
                    state_embedding = torch.cat([state_embedding, padding], dim=-1)
                else:
                    # æˆªæ–­
                    state_embedding = state_embedding[:, :self.output_dim]
            
            # è·å–æœ‰æ•ˆåŠ¨ä½œ
            if valid_actions is None:
                valid_actions = self.get_valid_actions(state, **kwargs)
            
            # âœ… ä¿®å¤ï¼šç¡®ä¿æœ‰æ•ˆåŠ¨ä½œåˆ—è¡¨ä¸ä¸ºç©º
            if not valid_actions:
                print("âš ï¸ æ²¡æœ‰æœ‰æ•ˆåŠ¨ä½œï¼Œä½¿ç”¨éšæœºåŠ¨ä½œ")
                return np.random.randint(0, self.action_dim)
            
            # Îµ-è´ªå©ªç­–ç•¥
            if self.is_training and np.random.random() < self.epsilon:
                # æ¢ç´¢ï¼šä»æœ‰æ•ˆåŠ¨ä½œä¸­éšæœºé€‰æ‹©
                action = int(np.random.choice(valid_actions))
            else:
                # åˆ©ç”¨ï¼šé€‰æ‹©Qå€¼æœ€é«˜çš„æœ‰æ•ˆåŠ¨ä½œ
                self.policy_network.eval()
                with torch.no_grad():
                    q_values = self.policy_network(state_embedding)
                    
                    # å±è”½æ— æ•ˆåŠ¨ä½œ
                    masked_q_values = self.mask_invalid_actions(q_values, valid_actions)
                    action = int(masked_q_values.argmax(dim=1).item())
                
                if self.is_training:
                    self.policy_network.train()
            
            # âœ… ä¿®å¤ï¼šç¡®ä¿è¿”å›å€¼æ˜¯intç±»å‹
            assert isinstance(action, (int, np.integer)), f"åŠ¨ä½œå¿…é¡»æ˜¯intç±»å‹ï¼Œå®é™…æ˜¯{type(action)}"
            assert action in valid_actions, f"é€‰æ‹©çš„åŠ¨ä½œ{action}ä¸åœ¨æœ‰æ•ˆåŠ¨ä½œåˆ—è¡¨ä¸­"
            
            return action
            
        except Exception as e:
            print(f"âŒ åŠ¨ä½œé€‰æ‹©å¤±è´¥: {e}")
            self.stats["errors"].append(f"åŠ¨ä½œé€‰æ‹©é”™è¯¯: {e}")
            # è¿”å›å®‰å…¨çš„é»˜è®¤åŠ¨ä½œ
            valid_actions = valid_actions or list(range(self.action_dim))
            return int(valid_actions[0]) if valid_actions else 0
    
    def get_valid_actions(self, state: Union[Data, Dict], **kwargs) -> List[int]:
        """
        è·å–VNFåµŒå…¥åœºæ™¯ä¸‹çš„æœ‰æ•ˆåŠ¨ä½œ - ä¿®å¤ç‰ˆ
        """
        try:
            # åŸºç¡€å®ç°ï¼šæ‰€æœ‰åŠ¨ä½œéƒ½æœ‰æ•ˆ
            available_nodes = kwargs.get('available_nodes', list(range(self.action_dim)))
            resource_constraints = kwargs.get('resource_constraints', {})
            
            valid_actions = []
            for node in available_nodes:
                if 0 <= node < self.action_dim:  # âœ… ä¿®å¤ï¼šç¡®ä¿èŠ‚ç‚¹IDåœ¨æœ‰æ•ˆèŒƒå›´å†…
                    if self._check_node_feasibility(node, resource_constraints):
                        valid_actions.append(node)
            
            # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªæœ‰æ•ˆåŠ¨ä½œ
            if not valid_actions:
                valid_actions = [0]  # é»˜è®¤é€‰æ‹©èŠ‚ç‚¹0
                print("âš ï¸ æ²¡æœ‰æœ‰æ•ˆåŠ¨ä½œï¼Œä½¿ç”¨é»˜è®¤åŠ¨ä½œ0")
            
            return valid_actions
            
        except Exception as e:
            print(f"âŒ è·å–æœ‰æ•ˆåŠ¨ä½œå¤±è´¥: {e}")
            return [0]  # è¿”å›å®‰å…¨é»˜è®¤å€¼
    
    def _check_node_feasibility(self, node_id: int, constraints: Dict) -> bool:
        """æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦æ»¡è¶³VNFåµŒå…¥çº¦æŸ"""
        try:
            # ç®€åŒ–å®ç°ï¼Œå®é™…åº”ç”¨ä¸­éœ€è¦æ›´å¤æ‚çš„é€»è¾‘
            if 'min_cpu' in constraints:
                return True  # æš‚æ—¶æ€»æ˜¯è¿”å›True
            return True
        except Exception as e:
            print(f"âŒ èŠ‚ç‚¹å¯è¡Œæ€§æ£€æŸ¥å¤±è´¥: {e}")
            return True  # é»˜è®¤è®¤ä¸ºå¯è¡Œ
    
    def store_transition(self, state, action, reward, next_state, done, **kwargs):
        """
        å­˜å‚¨ç»éªŒåˆ°ä¼˜å…ˆçº§å›æ”¾ç¼“å†²åŒº - ä¿®å¤ç‰ˆ
        """
        try:
            # âœ… ä¿®å¤ï¼šç¡®ä¿åŠ¨ä½œæ˜¯intç±»å‹
            if isinstance(action, (list, np.ndarray)):
                action = int(action[0]) if len(action) > 0 else 0
            else:
                action = int(action)
            
            # è®¡ç®—åˆå§‹ä¼˜å…ˆçº§ï¼ˆå¯ä»¥åŸºäºå¥–åŠ±æˆ–TDè¯¯å·®ï¼‰
            priority = kwargs.get('priority', abs(float(reward)) + 1e-6)
            
            self.replay_buffer.add(
                state=state,
                action=action,
                reward=float(reward),
                next_state=next_state,
                done=bool(done),
                priority=priority,
                agent_id=self.agent_id,
                **kwargs
            )
            
        except Exception as e:
            print(f"âŒ ç»éªŒå­˜å‚¨å¤±è´¥: {e}")
            self.stats["errors"].append(f"ç»éªŒå­˜å‚¨é”™è¯¯: {e}")
    
    def learn(self) -> Dict[str, float]:
        """
        DDQNå­¦ä¹ æ›´æ–° - ä¿®å¤ç‰ˆï¼ˆè§£å†³ç»´åº¦é—®é¢˜ï¼‰
        """
        if len(self.replay_buffer) < self.batch_size:
            return {"loss": 0.0, "q_value": 0.0}
        
        try:
            # ä¼˜å…ˆçº§é‡‡æ ·
            batch_data = self.replay_buffer.sample(self.batch_size, device=self.device)
            
            # âœ… ä¿®å¤ï¼šæ£€æŸ¥è¿”å›çš„æ•°æ®æ ¼å¼
            if len(batch_data) == 7:  # ä¼˜å…ˆçº§å›æ”¾è¿”å›7ä¸ªå…ƒç´ 
                states, actions, rewards, next_states, dones, weights, indices = batch_data
            else:
                print(f"âš ï¸ ä¼˜å…ˆçº§å›æ”¾è¿”å›æ•°æ®æ ¼å¼å¼‚å¸¸: {len(batch_data)}ä¸ªå…ƒç´ ")
                return {"loss": 0.0, "q_value": 0.0}
            
            # âœ… è°ƒè¯•ï¼šæ£€æŸ¥åŸå§‹æ•°æ®ç»´åº¦
            print(f"ğŸ” åŸå§‹æ•°æ®ç»´åº¦æ£€æŸ¥:")
            print(f"   states: {states.shape if hasattr(states, 'shape') else type(states)}")
            print(f"   actions: {actions.shape if hasattr(actions, 'shape') else type(actions)}")
            
            # å¤„ç†çŠ¶æ€ç¼–ç 
            if isinstance(states, Data):
                # å›¾æ•°æ®ï¼šä½¿ç”¨GNNç¼–ç 
                state_embeddings = self.gnn_encoder(states)
                next_state_embeddings = self.gnn_encoder(next_states)
            else:
                # âœ… ä¿®å¤ï¼šç¡®ä¿çŠ¶æ€å¼ é‡æ˜¯æ­£ç¡®çš„2ç»´æ ¼å¼
                if isinstance(states, torch.Tensor):
                    state_embeddings = states
                    next_state_embeddings = next_states
                    
                    # ä¿®å¤ç»´åº¦é—®é¢˜
                    if state_embeddings.dim() == 3:
                        print(f"ğŸ”§ ä¿®å¤3ç»´çŠ¶æ€å¼ é‡: {state_embeddings.shape}")
                        state_embeddings = state_embeddings.view(state_embeddings.size(0), -1)
                        next_state_embeddings = next_state_embeddings.view(next_state_embeddings.size(0), -1)
                        print(f"   ä¿®å¤å: {state_embeddings.shape}")
                    elif state_embeddings.dim() == 1:
                        state_embeddings = state_embeddings.unsqueeze(0)
                        next_state_embeddings = next_state_embeddings.unsqueeze(0)
                else:
                    # è½¬æ¢ä¸ºå¼ é‡
                    state_embeddings = torch.stack([torch.tensor(s, dtype=torch.float32) for s in states]).to(self.device)
                    next_state_embeddings = torch.stack([torch.tensor(s, dtype=torch.float32) for s in next_states]).to(self.device)
            
            # âœ… æœ€ç»ˆç»´åº¦éªŒè¯
            print(f"ğŸ” å¤„ç†åçŠ¶æ€ç»´åº¦: {state_embeddings.shape}")
            expected_batch_size = self.batch_size
            if state_embeddings.size(0) != expected_batch_size:
                print(f"âš ï¸ æ‰¹æ¬¡å¤§å°ä¸åŒ¹é…: æœŸæœ›{expected_batch_size}, å®é™…{state_embeddings.size(0)}")
            
            if state_embeddings.size(1) != self.output_dim:
                print(f"âš ï¸ ç‰¹å¾ç»´åº¦ä¸åŒ¹é…: æœŸæœ›{self.output_dim}, å®é™…{state_embeddings.size(1)}")
                return {"loss": 0.0, "q_value": 0.0}
            
            # å½“å‰Qå€¼ - ç°åœ¨åº”è¯¥å¾—åˆ°æ­£ç¡®çš„2ç»´è¾“å‡º
            current_q_values = self.policy_network(state_embeddings)
            print(f"ğŸ” Qå€¼è¾“å‡ºç»´åº¦: {current_q_values.shape}")
            
            # âœ… éªŒè¯Qå€¼è¾“å‡ºç»´åº¦
            expected_q_shape = (expected_batch_size, self.action_dim)
            if current_q_values.shape != expected_q_shape:
                print(f"âŒ Qå€¼è¾“å‡ºç»´åº¦å¼‚å¸¸: æœŸæœ›{expected_q_shape}, å®é™…{current_q_values.shape}")
                return {"loss": 0.0, "q_value": 0.0}
            
            # âœ… ä¿®å¤ï¼šå¤„ç†åŠ¨ä½œç´¢å¼•ç»´åº¦é—®é¢˜
            try:
                if isinstance(actions, torch.Tensor):
                    action_indices = actions.long().to(self.device)
                    # ç¡®ä¿action_indicesæ˜¯1ç»´å¼ é‡
                    if action_indices.dim() > 1:
                        action_indices = action_indices.squeeze()
                    if action_indices.dim() == 0:
                        action_indices = action_indices.unsqueeze(0)
                else:
                    # å¤„ç†åˆ—è¡¨å½¢å¼çš„åŠ¨ä½œ
                    action_list = []
                    for a in actions:
                        if isinstance(a, (list, np.ndarray)):
                            action_list.append(int(a[0]) if len(a) > 0 else 0)
                        else:
                            action_list.append(int(a))
                    action_indices = torch.tensor(action_list, dtype=torch.long, device=self.device)
                
                # âœ… ç¡®ä¿åŠ¨ä½œç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
                action_indices = torch.clamp(action_indices, 0, self.action_dim - 1)
                
                print(f"ğŸ” åŠ¨ä½œç´¢å¼•ç»´åº¦: {action_indices.shape}")
                
                # âœ… ä¿®å¤gatheræ“ä½œçš„ç»´åº¦é—®é¢˜
                if current_q_values.dim() == 2 and action_indices.dim() == 1:
                    # current_q_values: [batch_size, action_dim]
                    # action_indices: [batch_size]
                    current_q = current_q_values.gather(1, action_indices.unsqueeze(1)).squeeze(1)
                    print(f"ğŸ” å½“å‰Qå€¼ç»´åº¦: {current_q.shape}")
                else:
                    print(f"âŒ ç»´åº¦ä¸åŒ¹é…: Qå€¼{current_q_values.shape}, åŠ¨ä½œ{action_indices.shape}")
                    return {"loss": 0.0, "q_value": 0.0}
                    
            except Exception as e:
                print(f"âŒ åŠ¨ä½œç´¢å¼•å¤„ç†å¤±è´¥: {str(e)}")
                print(f"   Qå€¼å½¢çŠ¶: {current_q_values.shape if 'current_q_values' in locals() else 'unknown'}")
                print(f"   åŠ¨ä½œç±»å‹: {type(actions)}")
                if hasattr(actions, 'shape'):
                    print(f"   åŠ¨ä½œå½¢çŠ¶: {actions.shape}")
                return {"loss": 0.0, "q_value": 0.0}
            
            # âœ… ä¿®å¤ï¼šåŒQå­¦ä¹ çš„ç›®æ ‡å€¼è®¡ç®—
            with torch.no_grad():
                try:
                    # ç­–ç•¥ç½‘ç»œé€‰æ‹©ä¸‹ä¸€çŠ¶æ€çš„æœ€ä¼˜åŠ¨ä½œ
                    next_q_values_policy = self.policy_network(next_state_embeddings)
                    next_actions = next_q_values_policy.argmax(dim=1)
                    
                    # âœ… ç¡®ä¿next_actionsç»´åº¦æ­£ç¡®
                    if next_actions.dim() == 1:
                        next_actions = next_actions.unsqueeze(1)
                    
                    # ç›®æ ‡ç½‘ç»œè¯„ä¼°é€‰å®šåŠ¨ä½œçš„ä»·å€¼
                    next_q_values_target = self.target_network(next_state_embeddings)
                    
                    # âœ… ä¿®å¤gatheræ“ä½œ
                    if next_q_values_target.dim() == 2 and next_actions.dim() == 2:
                        next_q = next_q_values_target.gather(1, next_actions).squeeze(1)
                    else:
                        print(f"âš ï¸ ç›®æ ‡Qå€¼ç»´åº¦ä¸åŒ¹é…: target_q{next_q_values_target.shape}, actions{next_actions.shape}")
                        next_q = next_q_values_target.max(dim=1)[0]  # å›é€€åˆ°æœ€å¤§å€¼
                    
                    # è®¡ç®—ç›®æ ‡Qå€¼
                    target_q = rewards + (self.gamma * next_q * ~dones)
                    
                except Exception as e:
                    print(f"âŒ ç›®æ ‡Qå€¼è®¡ç®—å¤±è´¥: {str(e)}")
                    # ç®€åŒ–çš„ç›®æ ‡Qå€¼è®¡ç®—
                    target_q = rewards + (self.gamma * torch.zeros_like(rewards))
            
            # è®¡ç®—TDè¯¯å·®
            td_errors = torch.abs(current_q - target_q)
            
            # âœ… ä¿®å¤ï¼šå¤„ç†é‡è¦æ€§é‡‡æ ·æƒé‡
            if weights is not None:
                loss = (weights * F.mse_loss(current_q, target_q, reduction='none')).mean()
            else:
                loss = F.mse_loss(current_q, target_q)
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(
                list(self.gnn_encoder.parameters()) + list(self.policy_network.parameters()),
                max_norm=1.0
            )
            
            self.optimizer.step()
            self.lr_scheduler.step()
            
            # æ›´æ–°ä¼˜å…ˆçº§
            if indices is not None:
                self.replay_buffer.update_priorities(indices, td_errors.detach().cpu().numpy())
            
            # æ›´æ–°ç›®æ ‡ç½‘ç»œ
            self.training_step += 1
            if self.training_step % self.target_update_freq == 0:
                self.update_target_network()
            
            # æ›´æ–°æ¢ç´¢ç‡
            self.decay_epsilon()
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            avg_q_value = current_q.mean().item()
            first_action = action_indices[0].item() if len(action_indices) > 0 else 0
            self.update_stats(
                reward=rewards.mean().item(),
                action=first_action,
                loss=loss.item(),
                q_values=torch.tensor([avg_q_value])
            )
            
            # âœ… æ–°å¢ï¼šå®šæœŸæ¸…ç†å†…å­˜
            if self.training_step % 100 == 0:
                self.cleanup_memory()
            
            return {
                "loss": loss.item(),
                "q_value": avg_q_value,
                "td_error": td_errors.mean().item(),
                "lr": self.lr_scheduler.get_last_lr()[0],
                "epsilon": self.epsilon
            }
            
        except Exception as e:
            print(f"âŒ DDQNå­¦ä¹ å¤±è´¥: {e}")
            self.stats["errors"].append(f"å­¦ä¹ é”™è¯¯: {e}")
            # æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            print(f"   é”™è¯¯è¯¦æƒ…: {str(e)}")
            if 'current_q_values' in locals():
                print(f"   Qå€¼å½¢çŠ¶: {current_q_values.shape}")
            if 'actions' in locals():
                print(f"   åŠ¨ä½œç±»å‹: {type(actions)}")
                if hasattr(actions, 'shape'):
                    print(f"   åŠ¨ä½œå½¢çŠ¶: {actions.shape}")
            return {"loss": 0.0, "q_value": 0.0, "error": str(e)}
        """
        DDQNå­¦ä¹ æ›´æ–° - ä¿®å¤ç‰ˆ
        """
        if len(self.replay_buffer) < self.batch_size:
            return {"loss": 0.0, "q_value": 0.0}
        
        try:
            # ä¼˜å…ˆçº§é‡‡æ ·
            batch_data = self.replay_buffer.sample(self.batch_size, device=self.device)
            
            # âœ… ä¿®å¤ï¼šæ£€æŸ¥è¿”å›çš„æ•°æ®æ ¼å¼
            if len(batch_data) == 7:  # ä¼˜å…ˆçº§å›æ”¾è¿”å›7ä¸ªå…ƒç´ 
                states, actions, rewards, next_states, dones, weights, indices = batch_data
            else:
                print(f"âš ï¸ ä¼˜å…ˆçº§å›æ”¾è¿”å›æ•°æ®æ ¼å¼å¼‚å¸¸: {len(batch_data)}ä¸ªå…ƒç´ ")
                return {"loss": 0.0, "q_value": 0.0}
            
            # å¤„ç†çŠ¶æ€ç¼–ç 
            if isinstance(states, Data):
                # å›¾æ•°æ®ï¼šä½¿ç”¨GNNç¼–ç 
                state_embeddings = self.gnn_encoder(states)
                next_state_embeddings = self.gnn_encoder(next_states)
            else:
                # å·²ç¼–ç çŠ¶æ€
                state_embeddings = states
                next_state_embeddings = next_states
            
            # âœ… ä¿®å¤ï¼šç¡®ä¿çŠ¶æ€åµŒå…¥æ˜¯æ­£ç¡®çš„2ç»´å¼ é‡
            if state_embeddings.dim() == 3:
                # å¦‚æœæ˜¯3ç»´ï¼Œå‹ç¼©æ‰ä¸­é—´ç»´åº¦
                state_embeddings = state_embeddings.squeeze(1)
                print(f"ğŸ”§ çŠ¶æ€åµŒå…¥ç»´åº¦ä¿®æ­£: 3ç»´ -> 2ç»´, æ–°å½¢çŠ¶: {state_embeddings.shape}")
            elif state_embeddings.dim() == 1:
                # å¦‚æœæ˜¯1ç»´ï¼Œæ·»åŠ batchç»´åº¦
                state_embeddings = state_embeddings.unsqueeze(0)
            
            if next_state_embeddings.dim() == 3:
                next_state_embeddings = next_state_embeddings.squeeze(1)
            elif next_state_embeddings.dim() == 1:
                next_state_embeddings = next_state_embeddings.unsqueeze(0)
            
            # âœ… éªŒè¯ç»´åº¦
            expected_shape = (len(experiences), self.output_dim)
            if state_embeddings.shape != expected_shape:
                print(f"âš ï¸ çŠ¶æ€åµŒå…¥ç»´åº¦å¼‚å¸¸: æœŸæœ›{expected_shape}, å®é™…{state_embeddings.shape}")
                return {"loss": 0.0, "q_value": 0.0}
            
            # âœ… ä¿®å¤ï¼šç¡®ä¿çŠ¶æ€ç»´åº¦æ­£ç¡®
            if state_embeddings.size(-1) != self.output_dim:
                print(f"âš ï¸ å­¦ä¹ æ—¶çŠ¶æ€ç»´åº¦ä¸åŒ¹é…: {state_embeddings.size(-1)} != {self.output_dim}")
                return {"loss": 0.0, "q_value": 0.0}
            
            # å½“å‰Qå€¼ - ç¡®ä¿è¾“å…¥æ˜¯2ç»´çš„
            current_q_values = self.policy_network(state_embeddings)
            
            # âœ… éªŒè¯Qå€¼è¾“å‡ºç»´åº¦
            expected_q_shape = (len(experiences), self.action_dim)
            if current_q_values.shape != expected_q_shape:
                print(f"âš ï¸ Qå€¼è¾“å‡ºç»´åº¦å¼‚å¸¸: æœŸæœ›{expected_q_shape}, å®é™…{current_q_values.shape}")
                return {"loss": 0.0, "q_value": 0.0}
            
            # âœ… ä¿®å¤ï¼šå¤„ç†åŠ¨ä½œç´¢å¼•ï¼Œç¡®ä¿ç±»å‹æ­£ç¡®
            if isinstance(actions, torch.Tensor):
                action_indices = actions.long()
            else:
                # å¤„ç†åˆ—è¡¨å½¢å¼çš„åŠ¨ä½œ
                action_list = []
                for a in actions:
                    if isinstance(a, (list, np.ndarray)):
                        action_list.append(int(a[0]) if len(a) > 0 else 0)
                    else:
                        action_list.append(int(a))
                action_indices = torch.tensor(action_list, dtype=torch.long, device=self.device)
            
            # âœ… ä¿®å¤ï¼šç¡®ä¿åŠ¨ä½œç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
            action_indices = torch.clamp(action_indices, 0, self.action_dim - 1)
            current_q = current_q_values.gather(1, action_indices.unsqueeze(1)).squeeze(1)
            
            # âœ… ä¿®å¤ï¼šåŒQå­¦ä¹ çš„ç›®æ ‡å€¼è®¡ç®—
            with torch.no_grad():
                try:
                    # ç­–ç•¥ç½‘ç»œé€‰æ‹©ä¸‹ä¸€çŠ¶æ€çš„æœ€ä¼˜åŠ¨ä½œ
                    next_q_values_policy = self.policy_network(next_state_embeddings)
                    next_actions = next_q_values_policy.argmax(dim=1)
                    
                    # âœ… ç¡®ä¿next_actionsç»´åº¦æ­£ç¡®
                    if next_actions.dim() == 1:
                        next_actions = next_actions.unsqueeze(1)
                    
                    # ç›®æ ‡ç½‘ç»œè¯„ä¼°é€‰å®šåŠ¨ä½œçš„ä»·å€¼
                    next_q_values_target = self.target_network(next_state_embeddings)
                    
                    # âœ… ä¿®å¤gatheræ“ä½œ
                    if next_q_values_target.dim() == 2 and next_actions.dim() == 2:
                        next_q = next_q_values_target.gather(1, next_actions).squeeze(1)
                    else:
                        print(f"âš ï¸ ç›®æ ‡Qå€¼ç»´åº¦ä¸åŒ¹é…: target_q{next_q_values_target.shape}, actions{next_actions.shape}")
                        next_q = next_q_values_target.max(dim=1)[0]  # å›é€€åˆ°æœ€å¤§å€¼
                    
                    # è®¡ç®—ç›®æ ‡Qå€¼
                    target_q = rewards + (self.gamma * next_q * ~dones)
                    
                except Exception as e:
                    print(f"âŒ ç›®æ ‡Qå€¼è®¡ç®—å¤±è´¥: {str(e)}")
                    # ç®€åŒ–çš„ç›®æ ‡Qå€¼è®¡ç®—
                    target_q = rewards + (self.gamma * torch.zeros_like(rewards))
            
            # è®¡ç®—TDè¯¯å·®
            td_errors = torch.abs(current_q - target_q)
            
            # âœ… ä¿®å¤ï¼šå¤„ç†é‡è¦æ€§é‡‡æ ·æƒé‡
            if weights is not None:
                loss = (weights * F.mse_loss(current_q, target_q, reduction='none')).mean()
            else:
                loss = F.mse_loss(current_q, target_q)
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(
                list(self.gnn_encoder.parameters()) + list(self.policy_network.parameters()),
                max_norm=1.0
            )
            
            self.optimizer.step()
            self.lr_scheduler.step()
            
            # æ›´æ–°ä¼˜å…ˆçº§
            if indices is not None:
                self.replay_buffer.update_priorities(indices, td_errors.detach().cpu().numpy())
            
            # æ›´æ–°ç›®æ ‡ç½‘ç»œ
            self.training_step += 1
            if self.training_step % self.target_update_freq == 0:
                self.update_target_network()
            
            # æ›´æ–°æ¢ç´¢ç‡
            self.decay_epsilon()
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            avg_q_value = current_q.mean().item()
            first_action = action_indices[0].item() if len(action_indices) > 0 else 0
            self.update_stats(
                reward=rewards.mean().item(),
                action=first_action,
                loss=loss.item(),
                q_values=torch.tensor([avg_q_value])
            )
            
            # âœ… æ–°å¢ï¼šå®šæœŸæ¸…ç†å†…å­˜
            if self.training_step % 100 == 0:
                self.cleanup_memory()
            
            return {
                "loss": loss.item(),
                "q_value": avg_q_value,
                "td_error": td_errors.mean().item(),
                "lr": self.lr_scheduler.get_last_lr()[0],
                "epsilon": self.epsilon
            }
            
        except Exception as e:
            print(f"âŒ DDQNå­¦ä¹ å¤±è´¥: {e}")
            self.stats["errors"].append(f"å­¦ä¹ é”™è¯¯: {e}")
            # æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            print(f"   é”™è¯¯è¯¦æƒ…: {str(e)}")
            if 'current_q_values' in locals():
                print(f"   Qå€¼å½¢çŠ¶: {current_q_values.shape}")
            if 'actions' in locals():
                print(f"   åŠ¨ä½œç±»å‹: {type(actions)}")
                if hasattr(actions, 'shape'):
                    print(f"   åŠ¨ä½œå½¢çŠ¶: {actions.shape}")
            return {"loss": 0.0, "q_value": 0.0, "error": str(e)}
    
    def compute_td_error(self, state, action, reward, next_state, done) -> float:
        """è®¡ç®—TDè¯¯å·®ç”¨äºä¼˜å…ˆçº§æ›´æ–° - ä¿®å¤ç‰ˆ"""
        try:
            with torch.no_grad():
                if isinstance(state, Data):
                    state_emb = self.gnn_encoder(state.to(self.device))
                    next_state_emb = self.gnn_encoder(next_state.to(self.device))
                else:
                    # âœ… ä¿®å¤ï¼šä½¿ç”¨detach().clone()æ›¿ä»£torch.tensor()
                    if isinstance(state, torch.Tensor):
                        state_emb = state.detach().clone().to(self.device)
                        if state_emb.dim() == 1:
                            state_emb = state_emb.unsqueeze(0)
                    else:
                        state_emb = torch.tensor(state, device=self.device, dtype=torch.float32).unsqueeze(0)
                    
                    if isinstance(next_state, torch.Tensor):
                        next_state_emb = next_state.detach().clone().to(self.device)
                        if next_state_emb.dim() == 1:
                            next_state_emb = next_state_emb.unsqueeze(0)
                    else:
                        next_state_emb = torch.tensor(next_state, device=self.device, dtype=torch.float32).unsqueeze(0)
                
                # âœ… ä¿®å¤ï¼šç¡®ä¿åŠ¨ä½œæ˜¯intå¹¶åœ¨æœ‰æ•ˆèŒƒå›´å†…
                action = int(action[0]) if isinstance(action, (list, np.ndarray)) else int(action)
                action = max(0, min(action, self.action_dim - 1))  # é™åˆ¶åœ¨æœ‰æ•ˆèŒƒå›´å†…
                
                # âœ… ä¿®å¤ï¼šç¡®ä¿çŠ¶æ€ç»´åº¦åŒ¹é…ç½‘ç»œæœŸæœ›
                if state_emb.size(-1) != self.output_dim:
                    print(f"âš ï¸ TDè¯¯å·®è®¡ç®—ï¼šçŠ¶æ€ç»´åº¦ä¸åŒ¹é… {state_emb.size(-1)} != {self.output_dim}")
                    return 1.0  # è¿”å›é»˜è®¤ä¼˜å…ˆçº§
                
                current_q_values = self.policy_network(state_emb)
                
                # âœ… ä¿®å¤ï¼šæ£€æŸ¥åŠ¨ä½œç´¢å¼•æ˜¯å¦è¶Šç•Œ
                if action >= current_q_values.size(1):
                    print(f"âš ï¸ TDè¯¯å·®è®¡ç®—ï¼šåŠ¨ä½œç´¢å¼•è¶Šç•Œ {action} >= {current_q_values.size(1)}")
                    return 1.0  # è¿”å›é»˜è®¤ä¼˜å…ˆçº§
                
                current_q = current_q_values[0, action]
                
                if not done:
                    next_q_values = self.policy_network(next_state_emb)
                    next_action = next_q_values.argmax(dim=1)
                    next_q_target = self.target_network(next_state_emb)
                    next_q = next_q_target.gather(1, next_action.unsqueeze(1)).squeeze()
                    target_q = reward + self.gamma * next_q
                else:
                    target_q = torch.tensor(reward, device=self.device, dtype=torch.float32)
                
                td_error = abs(current_q - target_q).item()
                
            return td_error
            
        except Exception as e:
            print(f"âŒ TDè¯¯å·®è®¡ç®—å¤±è´¥: {e}")
            return 1.0  # è¿”å›é»˜è®¤ä¼˜å…ˆçº§
    
    def save_model(self, filepath: str):
        """ä¿å­˜æ¨¡å‹ - ä¿®å¤ç‰ˆ"""
        try:
            torch.save({
                'gnn_encoder': self.gnn_encoder.state_dict(),
                'policy_network': self.policy_network.state_dict(),
                'target_network': self.target_network.state_dict(),
                'optimizer': self.optimizer.state_dict(),
                'training_step': self.training_step,
                'epsilon': self.epsilon,
                'config': self.config  # âœ… æ–°å¢ï¼šä¿å­˜é…ç½®
            }, filepath)
            print(f"ğŸ’¾ DDQNæ¨¡å‹å·²ä¿å­˜: {filepath}")
            
        except Exception as e:
            print(f"âŒ DDQNæ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
    
    def load_model(self, filepath: str):
        """åŠ è½½æ¨¡å‹ - ä¿®å¤ç‰ˆ"""
        try:
            checkpoint = torch.load(filepath, map_location=self.device)
            
            self.gnn_encoder.load_state_dict(checkpoint['gnn_encoder'])
            self.policy_network.load_state_dict(checkpoint['policy_network'])
            self.target_network.load_state_dict(checkpoint['target_network'])
            self.optimizer.load_state_dict(checkpoint['optimizer'])
            self.training_step = checkpoint['training_step']
            self.epsilon = checkpoint['epsilon']
            
            print(f"ğŸ“‚ DDQNæ¨¡å‹å·²åŠ è½½: {filepath}")
            
        except Exception as e:
            print(f"âŒ DDQNæ¨¡å‹åŠ è½½å¤±è´¥: {e}")


def test_ddqn_agent():
    """æµ‹è¯•DDQNæ™ºèƒ½ä½“ - ä¿®å¤ç‰ˆ"""
    print("ğŸ§ª æµ‹è¯•ä¿®å¤åçš„DDQNæ™ºèƒ½ä½“...")
    
    config = {
        "gnn": {"edge_aware": {"hidden_dim": 64, "output_dim": 256, "layers": 4}},
        "train": {"lr": 0.001, "gamma": 0.99, "batch_size": 16, "target_update": 10},
        "network": {"hidden_dim": 256}
    }
    
    try:
        agent = MultiDDQNAgent("test_ddqn", state_dim=8, action_dim=10, edge_dim=4, config=config)
        
        # æµ‹è¯•åŠ¨ä½œé€‰æ‹©
        test_state = torch.randn(1, 256)  # æ¨¡æ‹ŸGNNç¼–ç åçš„çŠ¶æ€
        action = agent.select_action(test_state)
        
        assert isinstance(action, (int, np.integer)), f"åŠ¨ä½œåº”è¯¥æ˜¯intç±»å‹ï¼Œå®é™…æ˜¯{type(action)}"
        assert 0 <= action < 10, f"åŠ¨ä½œåº”è¯¥åœ¨[0,9]èŒƒå›´å†…ï¼Œå®é™…æ˜¯{action}"
        print(f"âœ… åŠ¨ä½œé€‰æ‹©æµ‹è¯•: {action} (ç±»å‹: {type(action)})")
        
        # âœ… ä¿®å¤ï¼šæ·»åŠ æ›´å¤šç»éªŒä»¥ç¡®ä¿å­¦ä¹ ç¨³å®šï¼Œä½†ç¡®ä¿çŠ¶æ€ç»´åº¦æ­£ç¡®
        print("ğŸ“ æ·»åŠ è®­ç»ƒç»éªŒ...")
        for i in range(50):  # å¢åŠ ç»éªŒæ•°é‡
            # âœ… ç¡®ä¿çŠ¶æ€æ˜¯2ç»´çš„ï¼š[1, 256]
            state = torch.randn(1, 256)  # æ­£ç¡®çš„2ç»´çŠ¶æ€
            action = i % 10
            reward = np.random.random() - 0.5  # éšæœºå¥–åŠ±
            next_state = torch.randn(1, 256)  # æ­£ç¡®çš„2ç»´çŠ¶æ€
            done = False
            agent.store_transition(state, action, reward, next_state, done)
        
        print(f"âœ… ç»éªŒå­˜å‚¨æµ‹è¯•: ç¼“å†²åŒºå¤§å° {len(agent.replay_buffer)}")
        
        # âœ… ä¿®å¤ï¼šç¡®ä¿æœ‰è¶³å¤Ÿçš„ç»éªŒè¿›è¡Œå­¦ä¹ 
        if len(agent.replay_buffer) >= agent.batch_size:
            print("ğŸ“ å¼€å§‹å­¦ä¹ æµ‹è¯•...")
            learning_info = agent.learn()
            print(f"âœ… å­¦ä¹ æµ‹è¯•: Loss={learning_info.get('loss', 0):.4f}, Qå€¼={learning_info.get('q_value', 0):.4f}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
            if learning_info.get('error'):
                print(f"âš ï¸ å­¦ä¹ è¿‡ç¨‹ä¸­çš„é”™è¯¯: {learning_info['error']}")
            else:
                print("âœ… å­¦ä¹ è¿‡ç¨‹æ— é”™è¯¯")
        else:
            print(f"âš ï¸ ç»éªŒä¸è¶³ï¼Œæ— æ³•è¿›è¡Œå­¦ä¹ æµ‹è¯• ({len(agent.replay_buffer)} < {agent.batch_size})")
        
        # æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯
        stats = agent.get_stats()
        print(f"âœ… ç»Ÿè®¡æµ‹è¯•: é”™è¯¯æ•°é‡={stats.get('error_count', 0)}")
        
        # âœ… æµ‹è¯•å†…å­˜æ¸…ç†
        agent.cleanup_memory()
        print("âœ… å†…å­˜æ¸…ç†æµ‹è¯•é€šè¿‡")
        
        # âœ… æµ‹è¯•TDè¯¯å·®è®¡ç®—
        try:
            # åˆ›å»ºåŒ¹é…ç½‘ç»œæœŸæœ›ç»´åº¦çš„æµ‹è¯•æ•°æ®
            test_state_td = torch.randn(256)  # 1ç»´çŠ¶æ€ï¼Œå°†è¢«è½¬æ¢ä¸ºåˆé€‚çš„ç»´åº¦
            td_error = agent.compute_td_error(
                state=test_state_td,
                action=5,
                reward=1.0,
                next_state=test_state_td,
                done=False
            )
            print(f"âœ… TDè¯¯å·®è®¡ç®—æµ‹è¯•: {td_error:.4f}")
        except Exception as e:
            print(f"âš ï¸ TDè¯¯å·®è®¡ç®—æµ‹è¯•å¤±è´¥: {e}")
        
        print("âœ… DDQNæ™ºèƒ½ä½“æµ‹è¯•å®Œæˆ!")
        return True
        
    except Exception as e:
        print(f"âŒ DDQNæ™ºèƒ½ä½“æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    test_ddqn_agent()